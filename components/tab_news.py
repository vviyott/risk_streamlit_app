# components/tab_news.py
import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import os
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor
import hashlib
from datetime import datetime

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def fetch_articles_with_keyword(keyword=None, max_pages=5, max_articles=3):
    """íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ ìµœì‹  ê¸°ì‚¬ë“¤ì„ í¬ë¡¤ë§í•˜ì—¬ ì œëª©, ìš”ì•½, ë‚ ì§œ, ë§í¬, ì´ë¯¸ì§€ ì •ë³´ë¥¼ ë‹´ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    base_url = "https://www.thinkfood.co.kr/news/articleList.html?sc_section_code=S1N2&view_type=sm"
    results = []

    for page in range(1, max_pages + 1):
        url = f"{base_url}&page={page}"
        res = requests.get(url)
        if res.status_code != 200:
            continue

        soup = BeautifulSoup(res.text, "html.parser")
        articles = soup.select(".list-block")

        for article in articles:
            if len(results) >= max_articles:
                return results  # ê¸°ì‚¬ 3ê°œ ëª¨ì´ë©´ ë°”ë¡œ ë°˜í™˜

            title_tag = article.select_one(".list-titles a strong")
            link_tag = article.select_one(".list-titles a")
            summary_tag = article.select_one(".line-height-3-2x")
            date_tag = article.select_one(".list-dated")

            if not title_tag or not link_tag:
                continue

            title = title_tag.get_text(strip=True)
            
            # keyword í•„í„°ë§ (keywordê°€ ì£¼ì–´ì§„ ê²½ìš°ì—ë§Œ)
            if keyword and keyword not in title:
                continue

            link = "https://www.thinkfood.co.kr" + link_tag["href"]
            summary = (summary_tag.get_text(strip=True)[:200] + "...") if summary_tag else ""
            info = date_tag.get_text(strip=True) if date_tag else ""

            # ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
            img_url = None
            try:
                res_detail = requests.get(link)
                soup_detail = BeautifulSoup(res_detail.text, "html.parser")
                img_tag = soup_detail.select_one("figure img")
                if img_tag and "src" in img_tag.attrs:
                    src = img_tag["src"]
                    img_url = src if src.startswith("http") else "https://cdn.thinkfood.co.kr" + src
            except:
                pass # ì˜¤ë¥˜ ë¬´ì‹œí•˜ê³  ì´ë¯¸ì§€ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬

            results.append({
                "title": title,
                "summary": summary,
                "info": info,
                "link": link,
                "img_url": img_url
            })

    return results

def fetch_full_article_content(url, max_length=200):  # ê¸°ì‚¬ë‹¹ 200ìë¡œ ì œí•œ
    """ê¸°ì‚¬ URLì—ì„œ ì „ì²´ ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        res = requests.get(url, timeout=10)  # timeout ì¶”ê°€
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        
        # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
        content = ""
        
        # ë°©ë²• 1: div.user-snip ì‹œë„
        content_div = soup.select_one("article.article-view-body")
        if content_div:
            # ê´‘ê³ ë‚˜ ê´€ë ¨ ê¸°ì‚¬ ë§í¬ ì œê±°
            for unwanted in content_div.select('.ad, .related, .link-area, .photo-info'):
                unwanted.decompose()
            
            # ë³¸ë¬¸ p íƒœê·¸ë“¤ë§Œ ì¶”ì¶œ
            paragraphs = content_div.select('p')
            if paragraphs:
                content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            else:
                # p íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                content = content_div.get_text(strip=True)
        
        # ë°©ë²• 2: article íƒœê·¸ ì‹œë„
        if not content:
            article_tag = soup.select_one('article')
            if article_tag:
                paragraphs = article_tag.select('p')
                content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
        
        # ë°©ë²• 3: .article-content ê°™ì€ í´ë˜ìŠ¤ ì‹œë„
        if not content:
            for selector in ['.article-content', '.news-content', '.content']:
                content_area = soup.select_one(selector)
                if content_area:
                    paragraphs = content_area.select('p')
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
                    if content:
                        break
        
        if content:
            # ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
            content = re.sub(r'\s+', ' ', content).strip()
            
            # ê¸¸ì´ ì œí•œ ì ìš©
            if len(content) > max_length:
                content = content[:max_length] + "..."
                
            # ë„ˆë¬´ ì§§ì€ ë‚´ìš©ì€ ì œì™¸
            if len(content) > 100:
                return content
        return None
    except Exception as e:
        print(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
        return None

def get_articles_parallel(article_urls, max_workers=3):
    """ë³‘ë ¬ë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        contents = list(executor.map(fetch_full_article_content, article_urls))
    return [content for content in contents if content]

def generate_cache_key(article_titles):
    """ê¸°ì‚¬ ì œëª©ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„± (ë‚ ì§œë³„ë¡œ êµ¬ë¶„)"""
    today = datetime.now().strftime("%Y-%m-%d")
    titles_string = "||".join(sorted(article_titles))
    hash_object = hashlib.md5(f"{today}-{titles_string}".encode())
    return hash_object.hexdigest()

@st.cache_data(ttl=86400, show_spinner=False)
def get_daily_cached_summary(cache_key, article_urls, openai_api_key):
    """í•˜ë£¨ ë‹¨ìœ„ë¡œ ìºì‹±ë˜ëŠ” GPT ìš”ì•½ (ì œëª© ê¸°ë°˜ ìºì‹œ í‚¤ ì‚¬ìš©)"""
    print(f"ìºì‹œ í‚¤ë¡œ ìš”ì•½ ìƒì„±: {cache_key}")
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘
    contents = get_articles_parallel(article_urls, max_workers=3)

    if contents:
        # ê¸°ì‚¬ë‹¹ 200ì ì œí•œìœ¼ë¡œ ì „ì²´ ë³¸ë¬¸ ê²°í•© (10ê°œ * 200ì = ìµœëŒ€ 2000ì)
        combined = "\n\n".join(contents)
        return summarize_with_openai(combined, openai_api_key)
    else:
        return "ìš”ì•½í•  ê¸°ì‚¬ ë³¸ë¬¸ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

def summarize_with_openai(content, openai_api_key):
    """OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸êµ­ ì‹í’ˆ ì‹œì¥ ê¸°ì‚¬ ìš”ì•½"""
    try:
        from openai import OpenAI

        client = OpenAI(api_key=openai_api_key)

        prompt = f"""
ë‹¤ìŒì€ ìµœê·¼ ë¯¸êµ­ ì‹í’ˆ ì‹œì¥ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ ë³¸ë¬¸ì…ë‹ˆë‹¤.

ë‹¤ìŒ ì¡°ê±´ì„ ë°˜ë“œì‹œ ì§€ì¼œ ìš”ì•½ë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”:

1. í˜„ì¬ëŠ” 2026ë…„ì…ë‹ˆë‹¤. **2026ë…„ ì´í›„ì˜ ë‚´ìš©ë§Œ í¬í•¨**í•˜ê³ , 2025ë…„ ì´ì „ì˜ ìˆ˜ì¹˜, ì‚¬ë¡€, íŠ¸ë Œë“œëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
2. **ê¸°ì‚¬ì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©**í•˜ê³ , ì„ì˜ì˜ ì¶”ë¡ ì´ë‚˜ í—ˆêµ¬ì˜ ìˆ˜ì¹˜ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
3. ë¬¸ì²´ëŠ” ê°ê´€ì ì´ê³  ë¶€ë“œëŸ½ê²Œ, ì¤‘ë¦½ì  í†¤ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”. (~í•  ìˆ˜ ìˆë‹¤, ~ë¡œ ë³´ì¸ë‹¤ ë“±)
4. ê°ê°ì˜ í•­ëª©ì€ ë‹¨ë¬¸ ë‚˜ì—´ì´ ì•„ë‹ˆë¼ **ë¶€ë“œëŸ¬ìš´ ì—°ê²°ì–´ì™€ ë¬¸ì¥ íë¦„**ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
5. ë…ìì—ê²Œ 'ì„¤ëª…í•´ì£¼ëŠ” ëŠë‚Œ'ìœ¼ë¡œ, **ì „ë¬¸ì„±ì´ ìˆìœ¼ë©´ì„œë„ ê³¼í•˜ì§€ ì•Šì€ ë”°ëœ»í•œ ì–´ì¡°**ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

---

ìš”ì•½ì€ ì•„ë˜ 4ê°œì˜ í•­ëª©ìœ¼ë¡œ ì‘ì„±í•˜ë©°, ê° í•­ëª©ì€ 2~3ë¬¸ì¥ ë‚´ì™¸ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤:

**ì‹œì¥ í™˜ê²½ ë³€í™”**  
**ì£¼ìš” íŠ¸ë Œë“œì™€ ì‚¬ë¡€**  
**ì‚°ì—… ê³¼ì œ ë° ë¦¬ìŠ¤í¬**  
**ì‹œì‚¬ì  ë° ì „ë§**

ì „ì²´ëŠ” 6~9ë¬¸ì¥ ë‚´ì™¸ë¡œ êµ¬ì„±í•˜ë©°, **êµµì€ ì œëª©ìœ¼ë¡œ ë¬¸ë‹¨ì„ êµ¬ë¶„**í•˜ê³ ,  
ë³´ê³ ì„œì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•œ ì‹œì¥ ë¶„ì„ì„ ì‘ì„±í•©ë‹ˆë‹¤.

ê¸°ì‚¬ ë³¸ë¬¸:
{content}
        """

        system_msg = (
            "ë‹¹ì‹ ì€ ë¯¸êµ­ ì‹í’ˆ ì‚°ì—… ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. "
            "ê¸°ì‚¬ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œì¥ ë³€í™”, íŠ¸ë Œë“œ, ë¦¬ìŠ¤í¬, ì‹œì‚¬ì ì„ ì—°ê²°í•´ í†µì°°ë ¥ ìˆê²Œ ìš”ì•½í•©ë‹ˆë‹¤. "
            "ì‚¬ë¡€ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ê³ , ì „ëµ ì œì•ˆì€ í˜„ì‹¤ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": prompt}
            ],
            max_tokens=700,
            temperature=0.3
        )

        return response.choices[0].message.content.strip()

    except Exception as e:
        return f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


def show_news():
    st.info("""
    ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í†µí•´ ì„¸ê³„ ì‹í’ˆ ì‹œì¥ì˜ íë¦„ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n 
    AIê°€ ë¶„ì„í•œ ë¯¸êµ­ì˜ ì£¼ìš” ì´ìŠˆ ê´€ë ¨ ì¸ì‚¬ì´íŠ¸ë¥¼ í•¨ê»˜ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    openai_api_key = os.getenv('OPENAI_API_KEY')

    articles = fetch_articles_with_keyword(keyword=None, max_pages=5, max_articles=3)

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("**ğŸ“° ì„¸ê³„ ë‰´ìŠ¤ ê¸°ì‚¬**")
        for article in articles:
            summary = article["summary"]
            if article["img_url"]:
                cols = st.columns([1, 3])
                with cols[0]:
                    st.image(article["img_url"], use_container_width=True)
                with cols[1]:
                    st.markdown(f"**[{article['title']}]({article['link']})**", unsafe_allow_html=True)
                    st.markdown(summary)
                    st.caption(article["info"])
            else:
                st.markdown(f"**[{article['title']}]({article['link']})**", unsafe_allow_html=True)
                st.markdown(summary)
                st.caption(article["info"])
            st.markdown("---")
        st.caption("ì¶œì²˜: [ì‹í’ˆìŒë£Œì‹ ë¬¸](https://www.thinkfood.co.kr/)")

    with col2:
        st.markdown("**ğŸ“Š ë¯¸êµ­ ì‹í’ˆ ì‚°ì—… ë™í–¥ ìš”ì•½ ë¶„ì„ (by OpenAI)**")
        
        # ì§„í–‰ ìƒí™© í‘œì‹œìš© placeholder
        progress_placeholder = st.empty()
        
        # 1ë‹¨ê³„: ê¸°ì‚¬ ìˆ˜ì§‘
        with st.spinner("ğŸ” ìµœì‹  ë¯¸êµ­ ê´€ë ¨ ì‹ë£Œí’ˆ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            with progress_placeholder.container():
                progress_bar = st.progress(0.1)
            articles_for_summary = fetch_articles_with_keyword(keyword="ë¯¸êµ­", max_pages=10, max_articles=10)
        
        if articles_for_summary:
            # 2ë‹¨ê³„: ë³¸ë¬¸ ë¶„ì„ ì¤€ë¹„
            with st.spinner("ğŸ“„ ê¸°ì‚¬ ë³¸ë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                progress_placeholder.empty()
                with progress_placeholder.container():
                    progress_bar = st.progress(0.4)
                # ì œëª© ê¸°ë°˜ ìºì‹œ í‚¤ ìƒì„±
                article_titles = [a["title"] for a in articles_for_summary]
                cache_key = generate_cache_key(article_titles)
                article_urls = [a["link"] for a in articles_for_summary]

            # 3ë‹¨ê³„: AI ë¶„ì„
            with st.spinner("ğŸ¤– AIê°€ ì‹œì¥ ë™í–¥ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                progress_placeholder.empty()
                with progress_placeholder.container():
                    progress_bar = st.progress(0.7)
                summary_result = get_daily_cached_summary(cache_key, article_urls, openai_api_key)

            # 4ë‹¨ê³„: ì™„ë£Œ í‘œì‹œ
            progress_placeholder.empty()
            with progress_placeholder.container():
                st.success("âœ… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                progress_bar = st.progress(1.0)
            
            import time
            time.sleep(0.5)  # ì™„ë£Œ ë©”ì‹œì§€ë¥¼ 0.5ì´ˆê°„ í‘œì‹œ
            
            # ìµœì¢… ê²°ê³¼ í‘œì‹œ
            progress_placeholder.empty()
            st.success(summary_result)

            with st.expander("ğŸ” ìš”ì•½ì— ì‚¬ìš©ëœ ê¸°ì‚¬ ì¶œì²˜ ë³´ê¸°"):
                for article in articles_for_summary:
                    st.markdown(f"- [{article['title']}]({article['link']})", unsafe_allow_html=True)
        else:
            progress_placeholder.empty()

            st.warning("ë¯¸êµ­ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


