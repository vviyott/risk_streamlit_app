# utils/chat_common_functions.py
"""
ì±—ë´‡ ê³µí†µ ê¸°ëŠ¥ ëª¨ìŒ - ìµœì í™” ë²„ì „
- ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
- ëŒ€í™” ê¸°ë¡ ì €ì¥/ë¡œë“œ
- LangChain íˆìŠ¤í† ë¦¬ ë³€í™˜
- ê¸°íƒ€ ê³µí†µ ìœ í‹¸ë¦¬í‹°
"""
import streamlit as st
import json
import os
import glob
from datetime import datetime
from typing import List, Dict, Any, Optional
from langchain_core.messages import AIMessage, HumanMessage
import threading
from functools import lru_cache
import time

# ëŒ€í™” ê¸°ë¡ íŒŒì¼ ê²½ë¡œ
CHAT_HISTORY_FILE = "chat_histories.json"

# íŒŒì¼ ë½ ê°ì²´ (ë™ì‹œ ì ‘ê·¼ ë°©ì§€)
_file_lock = threading.Lock()

# ìºì‹œëœ íˆìŠ¤í† ë¦¬ ë°ì´í„°
@st.cache_data(ttl=60)  # 60ì´ˆ TTLë¡œ ìºì‹±
def _load_all_histories() -> Dict:
    """ëª¨ë“  ëŒ€í™” ê¸°ë¡ì„ ìºì‹œì™€ í•¨ê»˜ ë¡œë“œ"""
    try:
        if not os.path.exists(CHAT_HISTORY_FILE):
            return {}
            
        with open(CHAT_HISTORY_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        st.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

def save_chat_history(project_name: str, chat_history: List, langchain_history: List, chat_mode: str) -> bool:
    """í”„ë¡œì íŠ¸ ëŒ€í™” ê¸°ë¡ì„ JSON íŒŒì¼ì— ì €ì¥ - ìµœì í™” ë²„ì „"""
    try:
        # íŒŒì¼ ë½ ì‚¬ìš©ìœ¼ë¡œ ë™ì‹œ ì ‘ê·¼ ë°©ì§€
        with _file_lock:
            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (ìºì‹œ ë¬´íš¨í™”)
            st.cache_data.clear()  # ìºì‹œ í´ë¦¬ì–´
            all_histories = _load_all_histories()
            
            # í”„ë¡œì íŠ¸ ë°ì´í„° ì—…ë°ì´íŠ¸ - ëª¨ë“œë³„ë¡œ ë¶„ë¦¬ ì €ì¥
            project_key = f"{project_name}_{chat_mode}"
            
            # LangChain íˆìŠ¤í† ë¦¬ ì§ë ¬í™” ìµœì í™”
            serialized_langchain = []
            if langchain_history:
                for msg in langchain_history:
                    msg_type = "HumanMessage" if isinstance(msg, HumanMessage) else "AIMessage"
                    serialized_langchain.append({
                        "type": msg_type, 
                        "content": msg.content
                    })
            
            all_histories[project_key] = {
                "last_updated": datetime.now().isoformat(),
                "chat_mode": chat_mode,
                "chat_history": chat_history,
                "langchain_history": serialized_langchain
            }
            
            # íŒŒì¼ ì €ì¥ (ì›ìì  ì“°ê¸°)
            temp_file = f"{CHAT_HISTORY_FILE}.tmp"
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(all_histories, f, ensure_ascii=False, indent=2)
            
            # ì›ìì  íŒŒì¼ êµì²´
            os.replace(temp_file, CHAT_HISTORY_FILE)
            
            return True
    except Exception as e:
        st.error(f"ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def load_chat_history(project_name: str, chat_mode: str) -> Optional[Dict]:
    """í”„ë¡œì íŠ¸ ëŒ€í™” ê¸°ë¡ ë¡œë“œ - ìºì‹œ í™œìš©"""
    try:
        all_histories = _load_all_histories()
        project_key = f"{project_name}_{chat_mode}"
        return all_histories.get(project_key)
    except Exception as e:
        st.error(f"ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return None

@lru_cache(maxsize=128)  # LRU ìºì‹œë¡œ ë©”ì‹œì§€ ê°ì²´ ì¬ìƒì„± ë°©ì§€
def _create_message_object(msg_type: str, content: str):
    """ë©”ì‹œì§€ ê°ì²´ ìƒì„± - ìºì‹œ ì ìš©"""
    if msg_type == "HumanMessage":
        return HumanMessage(content=content)
    elif msg_type == "AIMessage":
        return AIMessage(content=content)
    return None

def restore_langchain_history(langchain_data: List[Dict]) -> List:
    """JSONì—ì„œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ LangChain ë©”ì‹œì§€ ê°ì²´ë¡œ ë³€í™˜ - ìµœì í™”"""
    if not langchain_data:
        return []
    
    restored = []
    try:
        for msg_data in langchain_data:
            msg_obj = _create_message_object(msg_data["type"], msg_data["content"])
            if msg_obj:
                restored.append(msg_obj)
    except Exception as e:
        print(f"LangChain íˆìŠ¤í† ë¦¬ ë³µì› ì‹¤íŒ¨: {e}")
    
    return restored

# ì„¸ì…˜ í‚¤ ìƒì„±ë„ ìºì‹œ ì ìš©
@lru_cache(maxsize=32)
def get_session_keys(chat_mode: str) -> Dict[str, str]:
    """ì±—ë´‡ ëª¨ë“œë³„ ì„¸ì…˜ ìƒíƒœ í‚¤ ìƒì„± - ìºì‹œ ì ìš©"""
    return {
        "chat_history": f"chat_history_{chat_mode}",
        "langchain_history": f"langchain_history_{chat_mode}",
        "project_name": f"current_project_name_{chat_mode}",
        "is_processing": f"is_processing_{chat_mode}",
        "selected_question": f"selected_question_{chat_mode}"
    }

def initialize_session_state(session_keys: Dict[str, str]) -> None:
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” - ì¡°ê±´ ì²´í¬ ìµœì í™”"""
    # ë”•ì…”ë„ˆë¦¬ ì»´í”„ë¦¬í—¨ì…˜ìœ¼ë¡œ í•œ ë²ˆì— ì²˜ë¦¬
    defaults = {
        session_keys["selected_question"]: "",
        session_keys["is_processing"]: False,
        session_keys["chat_history"]: [],
        session_keys["langchain_history"]: [],
        session_keys["project_name"]: ""
    }
    
    for key, default_value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

def clear_session_state(session_keys: Dict[str, str]) -> None:
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ëŒ€í™” ê¸°ë¡ ì‚­ì œ) - ë°°ì¹˜ ì²˜ë¦¬"""
    # í•œ ë²ˆì— ì—¬ëŸ¬ ìƒíƒœ ì—…ë°ì´íŠ¸
    updates = {
        session_keys["chat_history"]: [],
        session_keys["langchain_history"]: [],
        session_keys["is_processing"]: False,
        session_keys["selected_question"]: ""
    }
    
    for key, value in updates.items():
        st.session_state[key] = value

def handle_project_change(project_name: str, chat_mode: str, session_keys: Dict[str, str]) -> bool:
    """í”„ë¡œì íŠ¸ ë³€ê²½ ì²˜ë¦¬ - ì¡°ê±´ ì²´í¬ ìµœì í™”"""
    current_project = st.session_state.get(session_keys["project_name"], "")
    
    # í”„ë¡œì íŠ¸ ë³€ê²½ì´ ì—†ìœ¼ë©´ ë¹ ë¥´ê²Œ ë°˜í™˜
    if not project_name or project_name == current_project:
        return False
    
    # í”„ë¡œì íŠ¸ ë³€ê²½ ì²˜ë¦¬
    st.session_state[session_keys["project_name"]] = project_name
    
    # ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    project_data = load_chat_history(project_name, chat_mode)
    
    if project_data:
        # ëŒ€í™” ê¸°ë¡ ë³µì›
        st.session_state[session_keys["chat_history"]] = project_data.get("chat_history", [])
        
        # LangChain íˆìŠ¤í† ë¦¬ ë³µì›
        langchain_data = project_data.get("langchain_history", [])
        if langchain_data:
            st.session_state[session_keys["langchain_history"]] = restore_langchain_history(langchain_data)
        else:
            st.session_state[session_keys["langchain_history"]] = []
        
        st.success(f"'{project_name}' ({chat_mode}) í”„ë¡œì íŠ¸ì˜ ì´ì „ ëŒ€í™”ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    else:
        # ìƒˆ í”„ë¡œì íŠ¸ì¸ ê²½ìš° ê¸°ë¡ ì´ˆê¸°í™”
        st.session_state[session_keys["chat_history"]] = []
        st.session_state[session_keys["langchain_history"]] = []
        st.success(f"'{project_name}' ({chat_mode}) ìƒˆ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    return True

def display_chat_history(session_keys: Dict[str, str]) -> None:
    """ëŒ€í™” ê¸°ë¡ ì¶œë ¥ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë Œë”ë§"""
    chat_history = st.session_state.get(session_keys["chat_history"], [])
    
    # ë¹ˆ íˆìŠ¤í† ë¦¬ëŠ” ë¹ ë¥´ê²Œ ë°˜í™˜
    if not chat_history:
        return
    
    # ë©”ì‹œì§€ ì¶œë ¥
    for msg in chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

def update_chat_history(question: str, answer: str, session_keys: Dict[str, str], chat_history: List) -> None:
    """ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸ - ë°°ì¹˜ ì²˜ë¦¬"""
    # ìƒˆ ë©”ì‹œì§€ë“¤ì„ í•œ ë²ˆì— ì¶”ê°€
    new_messages = [
        {"role": "user", "content": question},
        {"role": "assistant", "content": answer}
    ]
    
    # í˜„ì¬ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    current_history = st.session_state.get(session_keys["chat_history"], [])
    current_history.extend(new_messages)
    
    # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
    st.session_state[session_keys["chat_history"]] = current_history
    st.session_state[session_keys["langchain_history"]] = chat_history

def handle_example_question(question: str, session_keys: Dict[str, str]) -> None:
    """ì˜ˆì‹œ ì§ˆë¬¸ ì²˜ë¦¬ - ë°°ì¹˜ ì—…ë°ì´íŠ¸"""
    # í•œ ë²ˆì— ì—¬ëŸ¬ ìƒíƒœ ì—…ë°ì´íŠ¸
    st.session_state.update({
        session_keys["selected_question"]: question,
        session_keys["is_processing"]: True
    })

def handle_user_input(user_input: str, session_keys: Dict[str, str]) -> None:
    """ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ - ë°°ì¹˜ ì—…ë°ì´íŠ¸"""
    # í•œ ë²ˆì— ì—¬ëŸ¬ ìƒíƒœ ì—…ë°ì´íŠ¸
    st.session_state.update({
        session_keys["selected_question"]: user_input,
        session_keys["is_processing"]: True
    })

def reset_processing_state(session_keys: Dict[str, str]) -> None:
    """ì²˜ë¦¬ ìƒíƒœ ë¦¬ì…‹ - ë°°ì¹˜ ì—…ë°ì´íŠ¸"""
    # í•œ ë²ˆì— ì—¬ëŸ¬ ìƒíƒœ ì—…ë°ì´íŠ¸
    st.session_state.update({
        session_keys["selected_question"]: "",
        session_keys["is_processing"]: False
    })

# ì¶”ê°€ ìµœì í™” í•¨ìˆ˜ë“¤
def get_project_list() -> List[str]:
    """í”„ë¡œì íŠ¸ ëª©ë¡ ì¡°íšŒ - ìºì‹œ ì ìš©"""
    try:
        all_histories = _load_all_histories()
        projects = set()
        for project_key in all_histories.keys():
            # í”„ë¡œì íŠ¸ëª…ê³¼ ëª¨ë“œ ë¶„ë¦¬
            if '_' in project_key:
                project_name = '_'.join(project_key.split('_')[:-1])
                projects.add(project_name)
        return sorted(list(projects))
    except Exception:
        return []

def stream_response_typing(sentences: List[str], placeholder, delay_between_sentences=0.8, char_delay=0.03):
    """ChatGPT ìŠ¤íƒ€ì¼ ìŠ¤íŠ¸ë¦¬ë° íƒ€ì´í•‘ ì• ë‹ˆë©”ì´ì…˜"""
    if not sentences:
        return
    
    displayed_text = ""
    
    for sentence in sentences:
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ íƒ€ì´í•‘
        sentence_text = ""
        for char in sentence:
            sentence_text += char
            current_display = displayed_text + sentence_text + "â–Š"
            placeholder.markdown(current_display)
            time.sleep(char_delay)
        
        # ì™„ì„±ëœ ë¬¸ì¥ì„ ì „ì²´ í…ìŠ¤íŠ¸ì— ì¶”ê°€
        displayed_text += sentence + " "
        
        # ë¬¸ì¥ ê°„ ë”œë ˆì´
        if sentence != sentences[-1]:  # ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ì•„ë‹ˆë©´
            placeholder.markdown(displayed_text + "â–Š")
            time.sleep(delay_between_sentences)
    
    # ìµœì¢… í…ìŠ¤íŠ¸ ì¶œë ¥ (ì»¤ì„œ ì œê±°)
    placeholder.markdown(displayed_text.strip())

def quick_stream_response(text: str, placeholder, chunk_size=15, delay=0.5):
    """ë¹ ë¥¸ ì²­í¬ ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° (ê¸´ ë‹µë³€ìš©)"""
    words = text.split()
    chunks = []
    
    # ë‹¨ì–´ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
    for i in range(0, len(words), chunk_size):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
    
    displayed_text = ""
    for chunk in chunks:
        displayed_text += chunk + " "
        placeholder.markdown(displayed_text + "â–Š")
        time.sleep(delay)
    
    # ìµœì¢… ì¶œë ¥
    placeholder.markdown(displayed_text.strip())

def handle_streaming_response(result: Dict, placeholder, use_quick_mode=False):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ - ì‚¬ìš©ì ì„¤ì • ì™„ì „ ë°˜ì˜ ë²„ì „"""
    import streamlit as st
    import time
    
    try:
        # ì‚¬ìš©ì ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ í¬í•¨)
        settings = st.session_state.get("animation_settings", {})
        char_delay = settings.get("char_delay", 0.03)
        sentence_delay = settings.get("sentence_delay", 0.8)
        enabled = settings.get("enabled", True)
        quick_threshold = settings.get("quick_mode_threshold", 2000)
        debug_mode = st.session_state.get("debug_mode", False)
        
        # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
        if debug_mode:
            st.caption(f"ğŸ”§ ë””ë²„ê·¸: ì• ë‹ˆë©”ì´ì…˜={'ON' if enabled else 'OFF'}, ì†ë„={char_delay}s, ë¬¸ì¥ë”œë ˆì´={sentence_delay}s")
        
        # ì• ë‹ˆë©”ì´ì…˜ì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ì¦‰ì‹œ ì¶œë ¥
        if not enabled:
            placeholder.markdown(result["answer"])
            if debug_mode:
                st.success("âš¡ ì¦‰ì‹œ ì¶œë ¥ ëª¨ë“œë¡œ í‘œì‹œ ì™„ë£Œ")
            return
        
        # ë‹µë³€ ê¸¸ì´ ì²´í¬
        answer_text = result.get("answer", "")
        answer_length = len(answer_text)
        
        # ë¹ ë¥¸ ëª¨ë“œ ì¡°ê±´ ì²´í¬
        force_quick_mode = use_quick_mode or answer_length > quick_threshold
        
        if debug_mode:
            st.caption(f"ğŸ“ ë‹µë³€ ê¸¸ì´: {answer_length}ì, ë¹ ë¥¸ëª¨ë“œ: {'ON' if force_quick_mode else 'OFF'}")
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì„ íƒ ë° ì‹¤í–‰
        if "streaming_sentences" in result and result["streaming_sentences"] and not force_quick_mode:
            # ë°©ì‹ 1: ë¬¸ì¥ ë‹¨ìœ„ ì •ë°€ ìŠ¤íŠ¸ë¦¬ë° (ì¼ë°˜ ëª¨ë“œ)
            _stream_response_typing_enhanced(
                result["streaming_sentences"], 
                placeholder,
                char_delay=char_delay,
                sentence_delay=sentence_delay,
                debug_mode=debug_mode
            )
        else:
            # ë°©ì‹ 2: ë¹ ë¥¸ ì²­í¬ ìŠ¤íŠ¸ë¦¬ë° (ê¸´ ë‹µë³€ ë˜ëŠ” fallback)
            chunk_size = _calculate_optimal_chunk_size(answer_length)
            chunk_delay = max(0.1, char_delay * 20)  # ì²­í¬ ë”œë ˆì´ëŠ” ë¬¸ì ë”œë ˆì´ì˜ 20ë°°
            
            _quick_stream_response_enhanced(
                answer_text, 
                placeholder, 
                chunk_size=chunk_size,
                delay=chunk_delay,
                debug_mode=debug_mode
            )
            
    except Exception as e:
        # ì˜¤ë¥˜ ì‹œ ì¦‰ì‹œ ì¶œë ¥
        placeholder.markdown(result.get("answer", "ë‹µë³€ì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
        if debug_mode:
            st.error(f"ğŸš¨ ìŠ¤íŠ¸ë¦¬ë° ì• ë‹ˆë©”ì´ì…˜ ì˜¤ë¥˜: {e}")
        else:
            print(f"ìŠ¤íŠ¸ë¦¬ë° ì• ë‹ˆë©”ì´ì…˜ ì˜¤ë¥˜: {e}")

def _stream_response_typing_enhanced(sentences: List[str], placeholder, char_delay=0.03, sentence_delay=0.8, debug_mode=False):
    """í–¥ìƒëœ ë¬¸ì¥ ë‹¨ìœ„ íƒ€ì´í•‘ ì• ë‹ˆë©”ì´ì…˜"""
    if not sentences:
        return
    
    displayed_text = ""
    total_sentences = len(sentences)
    
    if debug_mode:
        st.caption(f"ğŸ¬ ë¬¸ì¥ë³„ íƒ€ì´í•‘ ì‹œì‘: {total_sentences}ê°œ ë¬¸ì¥")
    
    for sentence_idx, sentence in enumerate(sentences):
        if not sentence.strip():  # ë¹ˆ ë¬¸ì¥ ìŠ¤í‚µ
            continue
            
        # ë¬¸ì¥ë³„ íƒ€ì´í•‘
        sentence_text = ""
        sentence = sentence.strip()
        
        # ë¬¸ì¥ ì‹œì‘ ì‹œ ì•½ê°„ì˜ ë”œë ˆì´ (ì²« ë¬¸ì¥ ì œì™¸)
        if sentence_idx > 0:
            time.sleep(sentence_delay)
        
        # ë¬¸ìë³„ íƒ€ì´í•‘
        for char_idx, char in enumerate(sentence):
            sentence_text += char
            
            # í˜„ì¬ í‘œì‹œ í…ìŠ¤íŠ¸ ìƒì„± (ì»¤ì„œ í¬í•¨)
            current_display = displayed_text + sentence_text + "â–Š"
            placeholder.markdown(current_display)
            
            # ë¬¸ì ê°„ ë”œë ˆì´
            time.sleep(char_delay)
        
        # ì™„ì„±ëœ ë¬¸ì¥ì„ ì „ì²´ í…ìŠ¤íŠ¸ì— ì¶”ê°€
        displayed_text += sentence
        
        # ë¬¸ì¥ ëì— ê³µë°±ì´ë‚˜ ì¤„ë°”ê¿ˆì´ ì—†ìœ¼ë©´ ì¶”ê°€
        if sentence_idx < total_sentences - 1:
            if not displayed_text.endswith((' ', '\n')):
                displayed_text += " "
        
        # ì§„í–‰ìƒí™© í‘œì‹œ (ë””ë²„ê·¸ ëª¨ë“œ)
        if debug_mode and sentence_idx % 3 == 0:  # 3ë¬¸ì¥ë§ˆë‹¤ í‘œì‹œ
            progress = (sentence_idx + 1) / total_sentences
            st.caption(f"ğŸ“ ì§„í–‰ë¥ : {progress:.1%} ({sentence_idx + 1}/{total_sentences})")
    
    # ìµœì¢… í…ìŠ¤íŠ¸ ì¶œë ¥ (ì»¤ì„œ ì œê±°)
    placeholder.markdown(displayed_text.strip())
    
    if debug_mode:
        st.success(f"âœ… ë¬¸ì¥ë³„ íƒ€ì´í•‘ ì™„ë£Œ: {len(displayed_text)}ì ì¶œë ¥")

def _quick_stream_response_enhanced(text: str, placeholder, chunk_size=15, delay=0.3, debug_mode=False):
    """í–¥ìƒëœ ë¹ ë¥¸ ì²­í¬ ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°"""
    words = text.split()
    chunks = []
    
    # ë‹¨ì–´ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
    for i in range(0, len(words), chunk_size):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
    
    if debug_mode:
        st.caption(f"ğŸš€ ë¹ ë¥¸ ëª¨ë“œ ì‹œì‘: {len(chunks)}ê°œ ì²­í¬, ì²­í¬ë‹¹ {chunk_size}ë‹¨ì–´")
    
    displayed_text = ""
    total_chunks = len(chunks)
    
    for chunk_idx, chunk in enumerate(chunks):
        displayed_text += chunk + " "
        
        # ì»¤ì„œì™€ í•¨ê»˜ í‘œì‹œ
        current_display = displayed_text + "â–Š"
        placeholder.markdown(current_display)
        
        # ì²­í¬ ê°„ ë”œë ˆì´
        time.sleep(delay)
        
        # ì§„í–‰ìƒí™© í‘œì‹œ (ë””ë²„ê·¸ ëª¨ë“œ, 20% ê°„ê²©)
        if debug_mode and chunk_idx % max(1, total_chunks // 5) == 0:
            progress = (chunk_idx + 1) / total_chunks
            st.caption(f"ğŸƒ ë¹ ë¥¸ëª¨ë“œ ì§„í–‰ë¥ : {progress:.1%}")
    
    # ìµœì¢… ì¶œë ¥
    placeholder.markdown(displayed_text.strip())
    
    if debug_mode:
        st.success(f"âš¡ ë¹ ë¥¸ ëª¨ë“œ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ì¶œë ¥")

def _calculate_optimal_chunk_size(text_length: int) -> int:
    """í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ìµœì  ì²­í¬ í¬ê¸° ê³„ì‚°"""
    if text_length < 500:
        return 8      # ì§§ì€ í…ìŠ¤íŠ¸: ì‘ì€ ì²­í¬
    elif text_length < 1500:
        return 15     # ì¤‘ê°„ í…ìŠ¤íŠ¸: ë³´í†µ ì²­í¬
    elif text_length < 3000:
        return 25     # ê¸´ í…ìŠ¤íŠ¸: í° ì²­í¬
    else:
        return 35     # ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸: ë§¤ìš° í° ì²­í¬

def stream_response_typing(sentences: List[str], placeholder, delay_between_sentences=0.8, char_delay=0.03):
    """ê¸°ë³¸ ë¬¸ì¥ ë‹¨ìœ„ íƒ€ì´í•‘ ì• ë‹ˆë©”ì´ì…˜ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)"""
    return _stream_response_typing_enhanced(
        sentences, 
        placeholder, 
        char_delay=char_delay, 
        sentence_delay=delay_between_sentences,
        debug_mode=False
    )

def quick_stream_response(text: str, placeholder, chunk_size=15, delay=0.5):
    """ê¸°ë³¸ ë¹ ë¥¸ ì²­í¬ ìŠ¤íŠ¸ë¦¬ë° (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)"""
    return _quick_stream_response_enhanced(
        text, 
        placeholder, 
        chunk_size=chunk_size, 
        delay=delay,
        debug_mode=False
    )

def cleanup_old_histories(days_to_keep: int = 30) -> None:
    """ì˜¤ë˜ëœ ëŒ€í™” ê¸°ë¡ ì •ë¦¬ (ì„ íƒì‚¬í•­)"""
    try:
        all_histories = _load_all_histories()
        cutoff_date = datetime.now().timestamp() - (days_to_keep * 24 * 3600)
        
        cleaned_histories = {}
        for project_key, data in all_histories.items():
            try:
                last_updated = datetime.fromisoformat(data["last_updated"]).timestamp()
                if last_updated > cutoff_date:
                    cleaned_histories[project_key] = data
            except Exception:
                # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë³´ì¡´
                cleaned_histories[project_key] = data
        
        # ì •ë¦¬ëœ ë°ì´í„° ì €ì¥
        if len(cleaned_histories) < len(all_histories):
            with open(CHAT_HISTORY_FILE, 'w', encoding='utf-8') as f:
                json.dump(cleaned_histories, f, ensure_ascii=False, indent=2)
            st.cache_data.clear()  # ìºì‹œ í´ë¦¬ì–´
            
    except Exception as e:
        print(f"íˆìŠ¤í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")