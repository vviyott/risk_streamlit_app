# utils/chat_regulation.py

import json
import os
from functools import wraps
from dotenv import load_dotenv
from typing import TypedDict, List, Dict, Any 
from langchain_openai import OpenAIEmbeddings, ChatOpenAI 
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate 
from langchain_core.messages import AIMessage, HumanMessage
from langchain_community.chat_message_histories import ChatMessageHistory
from langgraph.graph import StateGraph, START, END
from langchain_teddynote import logging   # LangSmith ì¶”ì  í™œì„±í™”

load_dotenv()                   # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
logging.langsmith("LLMPROJECT") # LangSmith ì¶”ì  ì„¤ì •

class RegulationChatSystem: ###ì¶”ê°€
    """ê·œì œ ì±—ë´‡ ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.cache = {}  # ğŸ¬ ìºì‹œ ì €ì¥ì†Œ
        
    def _get_cache_key(self, question: str) -> str:
        """ì§ˆë¬¸ì„ ìºì‹œ í‚¤ë¡œ ë³€í™˜"""
        import re
        normalized = re.sub(r'[^\w\s]', '', question.lower().strip())
        return re.sub(r'\s+', '_', normalized)
    
    def process_question_with_cache(self, question: str, chat_history: List = None) -> Dict[str, Any]:
        """ìºì‹œë¥¼ ì ìš©í•œ ì§ˆë¬¸ ì²˜ë¦¬"""
        
        # ğŸ¬ ìºì‹œ ì²´í¬
        cache_key = self._get_cache_key(question)
        if cache_key in self.cache:
            print(f"ğŸ’¨ ê·œì œ ìºì‹œ ì‚¬ìš©: {question[:30]}...")
            return self.cache[cache_key]
        
        if chat_history is None:
            chat_history = []
        
        try:
            # ê¸°ì¡´ ask_question í•¨ìˆ˜ í˜¸ì¶œ
            result = graph.invoke({
                "question": question,
                "question_en": "",
                "chat_history": chat_history,
                "document_type": "",
                "categories": [],
                "context": "",
                "urls": [],
                "answer": "",
                "need_synthesis": False,
                "guidance_references": []
            })
            
            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_result = {
                "answer": result["answer"],
                "document_type": result["document_type"],
                "categories": result["categories"],
                "urls": result["urls"],
                "chat_history": result["chat_history"],
                "guidance_references": result["guidance_references"]
            }
            
            # ğŸ¬ ìºì‹œì— ì €ì¥
            self.cache[cache_key] = formatted_result
            return formatted_result
            
        except Exception as e:
            error_result = {
                "answer": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
                "document_type": "",
                "categories": [],
                "urls": [],
                "chat_history": chat_history,
                "guidance_references": []
            }
            return error_result  # ì—ëŸ¬ëŠ” ìºì‹œí•˜ì§€ ì•ŠìŒ

# ì „ì—­ ìºì‹± ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
_regulation_cache_system = None

def get_regulation_cache_system():
    """ê·œì œ ìºì‹± ì‹œìŠ¤í…œ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _regulation_cache_system
    if _regulation_cache_system is None:
        _regulation_cache_system = RegulationChatSystem()
    return _regulation_cache_system


# ê³„ì¸µì  êµ¬ì¡°ë¥¼ ìœ„í•œ ì¹´í…Œê³ ë¦¬ ê·¸ë£¹í•‘
CATEGORY_HIERARCHY = {
    "guidance": {
        "allergen": ["ì•ŒëŸ¬ì§€", "allergen", "ì•Œë ˆë¥´ê¸°", "ì•ŒëŸ¬ê²", "ê³¼ë¯¼ë°˜ì‘"],
        "additives": ["ì²¨ê°€ë¬¼", "additive", "ì‹í’ˆì²¨ê°€ë¬¼", "ë°©ë¶€ì œ", "ê°ë¯¸ë£Œ", "í–¥ë£Œ", "ì°©ìƒ‰ë£Œ"],
        "labeling": ["ë¼ë²¨ë§", "labeling", "ë¼ë²¨", "í‘œì‹œ", "ì˜ì–‘ì„±ë¶„", "ì›ì¬ë£Œ", "ì„±ë¶„í‘œì‹œ"],
        "main": ["ê°€ì´ë“œë¼ì¸", "guidance", "cpg", "ê°€ì´ë“œ", "ì¼ë°˜", "ì‹í’ˆê´€ë ¨", "food"]
    },
    "regulation": {
        "ecfr": ["ecfr", "ì—°ë°©ê·œì •ì§‘", "ì „ìì—°ë°©ê·œì •", "cfr"],
        "usc": ["21usc", "ë²•ë¥ ", "ì¡°í•­", "ê·œì •", "regulation", "ë²•ë ¹"]
    }
}

# í•œêµ­ì–´-ì˜ì–´ ë²ˆì—­ í•¨ìˆ˜
def translate_korean_to_english(korean_text: str) -> str:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
    try:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
        prompt = f"Translate the following Korean text to English. Only return the translation without any explanation:\n\n{korean_text}"
        response = llm.invoke([HumanMessage(content=prompt)])
        return response.content.strip()
    except Exception as e:
        print(f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return korean_text

# ChromaDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
def initialize_chromadb_collection():
    """ê¸°ì¡´ ChromaDB chroma_regulations ì»¬ë ‰ì…˜ì— ì—°ê²°"""
    try:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        # ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ì— ì—°ê²°
        vectorstore = Chroma(
            collection_name="chroma_regulations",  # ì‚¬ìš©ìê°€ ì§€ì •í•œ ì»¬ë ‰ì…˜ëª…
            embedding_function=embeddings,
            persist_directory="./data/chroma_db"
        )
        
        # ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ê³  ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        collection = vectorstore._collection
        document_count = collection.count()
        
        if document_count > 0:
            print(f"ChromaDB ì»¬ë ‰ì…˜ 'chroma_regulations' ì—°ê²° ì™„ë£Œ ({document_count}ê°œ ë¬¸ì„œ)")
            return vectorstore
        else:
            raise ValueError("ChromaDB ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.")
            
    except Exception as e:
        print(f"ChromaDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
        raise

# ì „ì—­ ë³€ìˆ˜ë¡œ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
vectorstore = initialize_chromadb_collection()

# ìƒíƒœ ì •ì˜
class GraphState(TypedDict):
    question: str
    question_en: str
    document_type: str
    categories: List[str]
    chat_history: List[HumanMessage | AIMessage]
    context: str
    urls: List[str]
    answer: str
    need_synthesis: bool
    guidance_references: List[str]  # guidanceì—ì„œ regulation ì°¸ì¡°ë¥¼ ìœ„í•œ í•„ë“œ

# ë…¸ë“œ ì •ì˜
def router_node(state: GraphState) -> GraphState:
    """ì´ˆê¸° ë¼ìš°íŒ…: guidance vs regulation ê²°ì • + ë²ˆì—­"""
    question = state["question"].lower()
    
    # í•œêµ­ì–´ ì§ˆë¬¸ì„ ì˜ì–´ë¡œ ë²ˆì—­
    try:
        question_en = translate_korean_to_english(state["question"])
        print(f"ë²ˆì—­ëœ ì§ˆë¬¸: {question_en}")
    except Exception as e:
        print(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")
        question_en = state["question"]
    
    # regulation í‚¤ì›Œë“œ ì²´í¬
    regulation_keywords = ["ë²•ë¥ ","ê·œì œ", "21usc", "ê·œì •", "regulation", "ë²•ë ¹", "ì¡°í•­", "cfr", "code of federal"]
    guidance_keywords = ["ê°€ì´ë“œ", "guidance", "cpg", "ì§€ì¹¨", "guideline"]
    
    combined_text = question + " " + question_en.lower()
    
    regulation_score = sum(1 for keyword in regulation_keywords if keyword in combined_text)
    guidance_score = sum(1 for keyword in guidance_keywords if keyword in combined_text)
    
    # ê¸°ë³¸ì ìœ¼ë¡œ guidance ìš°ì„ 
    document_type = "regulation" if regulation_score > guidance_score else "guidance"
    
    return {
        **state,
        "question_en": question_en,
        "document_type": document_type,
        "guidance_references": []
    }

def category_node(state: GraphState) -> GraphState:
    """ì¹´í…Œê³ ë¦¬ë³„ ì„¸ë¶€ ë¶„ë¥˜ - ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬"""
    question = state["question"].lower()
    question_en = state["question_en"].lower()
    doc_type = state["document_type"]
    
    # í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
    category_scores = {}
    category_keywords = CATEGORY_HIERARCHY[doc_type]
    
    # ì˜ì–´ í‚¤ì›Œë“œ ë§¤í•‘ í™•ì¥
    english_keywords = {
        "allergen": ["allergen", "allergy", "allergenic", "hypersensitivity", "allergic reaction"],
        "additives": ["additive", "preservatives", "sweetener", "flavoring", "coloring", "food additive"],
        "labeling": ["labeling", "label", "nutrition", "ingredient", "declaration", "nutritional facts"],
        "main": ["guidance", "general", "main", "comprehensive", "cpg", "food related"],
        "ecfr": ["electronic code", "federal regulations", "cfr", "code of federal regulations"],
        "usc": ["united states code", "federal law", "statute", "21 usc", "federal statute"]
    }
    
    # ê° ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
    for category, korean_keywords in category_keywords.items():
        score = 0
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in korean_keywords:
            if keyword.lower() in question:
                score += 2
        
        # ì˜ì–´ í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in english_keywords.get(category, []):
            if keyword in question_en:
                score += 1.5
        
        category_scores[category] = score
    
    # ë³µí•© ì§ˆë¬¸ ì²˜ë¦¬
    selected_categories = []
    
    # íŠ¹ë³„ íŒ¨í„´ ê°ì§€
    import re
    combined_text = question + " " + question_en.lower()
    
    complex_patterns = [
        (r'ì•ŒëŸ¬ì§€.*ê·œì œ|allergen.*regulation', 'allergen', 'guidance'),
        (r'ì²¨ê°€ë¬¼.*ê·œì œ|additive.*regulation', 'additives', 'guidance'), 
        (r'ë¼ë²¨ë§.*ê·œì œ|labeling.*regulation', 'labeling', 'guidance'),
    ]
    
    pattern_matched = False
    for pattern, target_category, target_doc_type in complex_patterns:
        if re.search(pattern, combined_text, re.IGNORECASE):
            selected_categories = [target_category]
            state["document_type"] = target_doc_type
            pattern_matched = True
            print(f"ë³µí•© ì§ˆë¬¸ ê°ì§€: '{target_category}' ì¹´í…Œê³ ë¦¬, '{target_doc_type}' ë¬¸ì„œíƒ€ì…ìœ¼ë¡œ ë³€ê²½")
            break
    
    if not pattern_matched:
        # ì¼ë°˜ ë¡œì§: ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ì¹´í…Œê³ ë¦¬ë“¤ ì„ íƒ
        if category_scores:
            max_score = max(category_scores.values())
            if max_score > 0:
                threshold = max_score * 0.7
                selected_categories = [cat for cat, score in category_scores.items() 
                                     if score >= threshold]
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    if not selected_categories:
        selected_categories = ["main"] if state["document_type"] == "guidance" else ["usc", "ecfr"]
    
    # ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ê°€ ì„ íƒë˜ë©´ ì¢…í•©ì´ í•„ìš”
    need_synthesis = len(selected_categories) > 1
    
    print(f"ì„ íƒëœ ì¹´í…Œê³ ë¦¬: {selected_categories}, ë¬¸ì„œíƒ€ì…: {state['document_type']}, ì ìˆ˜: {category_scores}")
    
    return {
        **state,
        "categories": selected_categories,
        "need_synthesis": need_synthesis
    }

def document_retrieval_node(state: GraphState) -> GraphState:
    all_documents = []; guidance_references = []; search_query = state["question_en"]
    for category in state["categories"]:
        try:
            filter_dict = {"$and": [{"document_type": {"$eq": state["document_type"]}}, {"category": {"$eq": category.lower()}}]}
            docs = vectorstore.as_retriever(search_kwargs={"k": 3, "filter": filter_dict}).invoke(search_query)
            if docs: all_documents.extend(docs)
        except Exception: continue
    if not all_documents: all_documents = vectorstore.as_retriever(search_kwargs={"k": 5}).invoke(search_query)
        
    unique_docs = list({doc.page_content[:100]: doc for doc in all_documents}.values())
    selected_docs = unique_docs[:5]
    
    unique_urls = sorted(list(set([doc.metadata.get("url", "") for doc in selected_docs if doc.metadata.get("url")])))
    url_to_number_map = {url: i + 1 for i, url in enumerate(unique_urls)}

    context_parts = []
    for doc in selected_docs:
        source_url = doc.metadata.get("url")
        if source_url and source_url in url_to_number_map:
            cite_num = url_to_number_map[source_url]
            context_part = f"[ì¶œì²˜ {cite_num}]: {doc.page_content}"
            context_parts.append(context_part)
        
    context = "\n\n---\n\n".join(context_parts)
    return { **state, "context": context, "urls": unique_urls, "guidance_references": [] }


def synthesis_node(state: GraphState) -> GraphState:
    """guidance â†’ regulation ë‹¨ë°©í–¥ ì°¸ì¡°ë¥¼ í†µí•œ ë‹µë³€ í’ˆì§ˆ í–¥ìƒ"""
    additional_context = ""
    additional_urls = []
    
    # guidance ë¬¸ì„œì—ì„œ regulation ì°¸ì¡°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹¤í–‰
    if state["document_type"] == "guidance" and state["guidance_references"]:
        try:
            print(f"regulation ì°¸ì¡° ê²€ìƒ‰ ì‹œì‘: {state['guidance_references']}")
            
            # ì°¸ì¡°ëœ regulation ì„¹ì…˜ë“¤ì„ ê²€ìƒ‰
            for reference in state["guidance_references"]:
                reference = reference.strip()
                if not reference:
                    continue
                
                # CFR ì°¸ì¡°ì¸ì§€ USC ì°¸ì¡°ì¸ì§€ íŒë‹¨
                ref_lower = reference.lower()
                if "cfr" in ref_lower or "21 cfr" in ref_lower:
                    target_category = "ecfr"
                elif "usc" in ref_lower or "21 u.s.c" in ref_lower:
                    target_category = "usc"
                else:
                    # ê¸°ë³¸ì ìœ¼ë¡œ ë‘˜ ë‹¤ ê²€ìƒ‰
                    target_category = None
                
                # regulation ë¬¸ì„œì—ì„œ í•´ë‹¹ ì°¸ì¡° ê²€ìƒ‰
                try:
                    if target_category:
                        # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë¡œ ê²€ìƒ‰
                        reg_filter = {
                            "$and": [
                                {"document_type": {"$eq": "regulation"}},
                                {"category": {"$eq": target_category}}
                            ]
                        }
                    else:
                        # regulation ë¬¸ì„œ ì „ì²´ì—ì„œ ê²€ìƒ‰
                        reg_filter = {"document_type": {"$eq": "regulation"}}
                    
                    reg_retriever = vectorstore.as_retriever(
                        search_kwargs={"k": 2, "filter": reg_filter}
                    )
                    
                    # ì°¸ì¡° ë²ˆí˜¸ë¥¼ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš©
                    reg_docs = reg_retriever.invoke(reference)
                    
                    if reg_docs:
                        ref_context = f"\n\n[{reference} ê´€ë ¨ ê·œì •]\n"
                        ref_context += "\n".join([doc.page_content[:500] + "..." for doc in reg_docs])
                        additional_context += ref_context
                        
                        ref_urls = [doc.metadata.get("url", "") for doc in reg_docs if doc.metadata.get("url")]
                        additional_urls.extend(ref_urls)
                        
                        print(f"ì°¸ì¡° '{reference}'ì—ì„œ {len(reg_docs)}ê°œ regulation ë¬¸ì„œ ë°œê²¬")
                    
                except Exception as e:
                    print(f"ì°¸ì¡° '{reference}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            # ì¼ë°˜ì ì¸ ê´€ë ¨ regulation ê²€ìƒ‰ (ì°¸ì¡°ê°€ êµ¬ì²´ì ì´ì§€ ì•Šì€ ê²½ìš°)
            if not additional_context:
                try:
                    search_query = state["question_en"]
                    reg_filter = {"document_type": {"$eq": "regulation"}}
                    reg_retriever = vectorstore.as_retriever(
                        search_kwargs={"k": 2, "filter": reg_filter}
                    )
                    reg_docs = reg_retriever.invoke(search_query)
                    
                    if reg_docs:
                        additional_context = "\n\n[ê´€ë ¨ ê·œì • ì°¸ì¡°]\n"
                        additional_context += "\n".join([doc.page_content[:500] + "..." for doc in reg_docs])
                        additional_urls = [doc.metadata.get("url", "") for doc in reg_docs if doc.metadata.get("url")]
                        print(f"ì¼ë°˜ regulation ê²€ìƒ‰ì—ì„œ {len(reg_docs)}ê°œ ë¬¸ì„œ ë°œê²¬")
                
                except Exception as e:
                    print(f"ì¼ë°˜ regulation ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        except Exception as e:
            print(f"guidance â†’ regulation ì°¸ì¡° ê²€ìƒ‰ ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
    
    # ì¢…í•©ì´ í•„ìš”í•œ ê²½ìš° (ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬)
    elif state["need_synthesis"]:
        try:
            search_query = state["question_en"]
            cross_filter = {"document_type": {"$eq": state["document_type"]}}
            cross_retriever = vectorstore.as_retriever(
                search_kwargs={"k": 2, "filter": cross_filter}
            )
            cross_docs = cross_retriever.invoke(search_query)
            
            if cross_docs:
                additional_context = "\n\n[ì¶”ê°€ ê´€ë ¨ ì •ë³´]\n"
                additional_context += "\n".join([doc.page_content[:500] + "..." for doc in cross_docs])
                additional_urls = [doc.metadata.get("url", "") for doc in cross_docs if doc.metadata.get("url")]
        
        except Exception as e:
            print(f"ì¢…í•© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ì™€ URL ë³‘í•©
    if additional_context:
        updated_context = state["context"] + additional_context
        updated_urls = state["urls"] + additional_urls
        
        return {
            **state,
            "context": updated_context,
            "urls": updated_urls
        }
    
    return state

def extract_domain_name(url: str) -> str:
    """URLì—ì„œ ì½ê¸° ì‰¬ìš´ ë„ë©”ì¸ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        
        if 'fda.gov' in domain:
            return 'FDA ê³µì‹ ì‚¬ì´íŠ¸'
        elif 'ecfr.gov' in domain:
            return 'eCFR ì „ìì—°ë°©ê·œì •ì§‘'
        elif 'cornell.edu' in domain:
            return 'Cornell Law School'
        else:
            return domain.capitalize()
    except:
        return "ê´€ë ¨ ì›¹ì‚¬ì´íŠ¸"

def generate_answer(state: GraphState) -> GraphState:
    """Perplexity ìŠ¤íƒ€ì¼ ì£¼ì„ì„ ìƒì„±í•˜ê³ , Pythonìœ¼ë¡œ ìµœì¢… ì¶œì²˜ ëª©ë¡ì„ í¬ë§·í•˜ëŠ” ë‹µë³€ ìƒì„±ê¸°"""
    
    source_list_str = "\n".join([f"[{i+1}] {url}" for i, url in enumerate(state["urls"])])
    
    # â–¼â–¼â–¼â–¼â–¼ 1. í”„ë¡¬í”„íŠ¸ ìˆ˜ì •: AIì—ê²Œ ì¶œì²˜ ëª©ë¡ ìƒì„± ì§€ì‹œë¥¼ ì‚­ì œ â–¼â–¼â–¼â–¼â–¼
    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë¯¸êµ­ FDA ê·œì œë¥¼ ì „ë¬¸ì ìœ¼ë¡œ í•´ì„í•˜ëŠ” ê·œì œ ìë¬¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì •ë°€í•˜ê³  ì‹ ë¢°ì„± ìˆëŠ” í•´ì„ì„ ì œê³µí•˜ì„¸ìš”.

â—ï¸í•µì‹¬ ê·œì¹™:
- ë°˜ë“œì‹œ ê·œì œ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨í•˜ê³ , ë¬¸ì„œ ë‚´ìš©ì„ ìµœëŒ€í•œ ë§ì´ ì‹£ì–´ì£¼ì„¸ìš”.
- **ê° í•­ëª©ì„ ì„¤ëª…í•  ë•Œ, ê·¸ ê·¼ê±°ê°€ ë˜ëŠ” ê·œì •ì˜ í•µì‹¬ ë‚´ìš©ì„ ìƒì„¸í•˜ê²Œ í’€ì–´ ì„¤ëª…í•´ì£¼ì„¸ìš”.** (ìµœì†Œ 2-3ë¬¸ì¥ ì´ìƒ)
- ì¤‘ìš”í•œ ì •ë³´ë‚˜ ê·œì •ì„ ì–¸ê¸‰í•  ë•Œë§ˆë‹¤ í•´ë‹¹í•˜ëŠ” ì¶œì²˜ ë²ˆí˜¸ë¥¼ [1], [2] í˜•íƒœë¡œ ë¬¸ì¥ ëì— ì‚½ì…í•˜ì„¸ìš”.
- ì¶œì²˜ê°€ í¬í•¨ëœ ì¡°í•­ì€ ì¸ìš© í‘œì‹œ(ì˜ˆ: 21 CFR 182.1)ë¡œ ëª…ì‹œí•˜ì„¸ìš”.
- ì¤‘ìš” ë‚´ìš©ì€ ë²ˆí˜¸ ëª©ë¡ í˜•ì‹ìœ¼ë¡œ ëª…í™•íˆ ì •ë¦¬í•˜ì„¸ìš”.
- ë§ˆì§€ë§‰ì—ëŠ” ìœ„ì˜ í•­ëª©ë“¤ì„ ìš”ì•½í•˜ì—¬ ì •ë¦¬í•œ ì¢…í•©ì  ë¶„ì„ ë¬¸ë‹¨ì„ ì¶”ê°€í•˜ì„¸ìš”.

ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸:
{question}
ğŸ“– ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ (ê° ë‚´ìš© ì•ì˜ [ì¶œì²˜ N]ì„ ë³´ê³  ì£¼ì„ì„ ë‹¬ì•„ì•¼ í•¨):
{context}
ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ì¶œì²˜ ëª©ë¡ (ì°¸ê³ ìš©):
{source_info}
ğŸ”½ ìœ„ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:"""
    )
    
    try:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)
        chain = prompt | llm | StrOutputParser()
        
        # AIëŠ” ë³¸ë¬¸ê³¼ ì¸ë¼ì¸ ì£¼ì„ê¹Œì§€ë§Œ ìƒì„±
        answer_text = chain.invoke({
            "question": state["question"],
            "context": state["context"],
            "source_info": source_list_str
        })
        
        # Python ì½”ë“œê°€ ì¸ë¼ì¸ ì£¼ì„ì„ í•˜ì´í¼ë§í¬ë¡œ ë³€í™˜
        final_answer_with_links = answer_text
        for i, url in enumerate(state["urls"]):
            final_answer_with_links = final_answer_with_links.replace(f"[{i+1}]", f" [[{i+1}]]({url})")

        # â–¼â–¼â–¼â–¼â–¼ 2. Python ì½”ë“œ ìˆ˜ì •: ì´ìƒì ì¸ í˜•íƒœë¡œ ì¶œì²˜ ëª©ë¡ì„ ì§ì ‘ ìƒì„±í•˜ì—¬ ì¶”ê°€ â–¼â–¼â–¼â–¼â–¼
        if state["urls"]:
            # extract_domain_name í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ìƒì ì¸ í¬ë§·ì˜ ì¶œì²˜ ëª©ë¡ì„ ìƒì„±
            url_text = "\n\nğŸ“ ì¶œì²˜:\n"
            for i, url in enumerate(state["urls"]):
                domain = extract_domain_name(url) # ì´ í•¨ìˆ˜ëŠ” generate_answer í•¨ìˆ˜ ë°–ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
                url_text += f"[{i+1}] [{domain}]({url})\n"
            
            # ìµœì¢…ì ìœ¼ë¡œ AI ë‹µë³€ê³¼ Pythonì´ ë§Œë“  ì¶œì²˜ ëª©ë¡ì„ ê²°í•©
            full_answer = f"{final_answer_with_links}{url_text}"
        else:
            full_answer = final_answer_with_links
        
        return { **state, "answer": full_answer }

    except Exception as e:
        return { **state, "answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}" }

def update_chat_history(state: GraphState) -> GraphState:
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
    try:
        current_history = state.get("chat_history", [])
        
        # ìƒˆ ë©”ì‹œì§€ ì¶”ê°€
        updated_history = current_history.copy()
        updated_history.append(HumanMessage(content=state["question"]))
        updated_history.append(AIMessage(content=state["answer"]))
        
        # íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ (ìµœëŒ€ 10ê°œ ë©”ì‹œì§€)
        if len(updated_history) > 10:
            updated_history = updated_history[-10:]
        
        return {
            **state,
            "chat_history": updated_history
        }
    
    except Exception as e:
        print(f"ì±„íŒ… íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return state

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(GraphState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("router", router_node)
workflow.add_node("category", category_node) 
workflow.add_node("retrieval", document_retrieval_node)
workflow.add_node("synthesis", synthesis_node)
workflow.add_node("generate", generate_answer)
workflow.add_node("update_history", update_chat_history)

# ì—£ì§€ ì¶”ê°€
workflow.add_edge(START, "router")
workflow.add_edge("router", "category")
workflow.add_edge("category", "retrieval")
workflow.add_edge("retrieval", "synthesis")
workflow.add_edge("synthesis", "generate")
workflow.add_edge("generate", "update_history")
workflow.add_edge("update_history", END)

# ê·¸ë˜í”„ ì»´íŒŒì¼
graph = workflow.compile()

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# def ask_question(question: str, chat_history: List = None) -> Dict[str, Any]:
#     """ì§ˆë¬¸ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
#     if chat_history is None:
#         chat_history = []
    
#     try:
#         result = graph.invoke({
#             "question": question,
#             "question_en": "",
#             "chat_history": chat_history,
#             "document_type": "",
#             "categories": [],
#             "context": "",
#             "urls": [],
#             "answer": "",
#             "need_synthesis": False,
#             "guidance_references": []
#         })
        
#         return {
#             "answer": result["answer"],
#             "document_type": result["document_type"],
#             "categories": result["categories"],
#             "urls": result["urls"],
#             "chat_history": result["chat_history"],
#             "guidance_references": result["guidance_references"]
#         }
    
#     except Exception as e:
#         return {
#             "answer": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
#             "document_type": "",
#             "categories": [],
#             "urls": [],
#             "chat_history": chat_history,
#             "guidance_references": []
#         }


def ask_question(question: str, chat_history: List = None) -> Dict[str, Any]:
    """ì§ˆë¬¸ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜ - ìºì‹± ì§€ì›"""
    
    # ğŸ¬ ìºì‹± ì‹œìŠ¤í…œ ì‚¬ìš©
    cache_system = get_regulation_cache_system()
    return cache_system.process_question_with_cache(question, chat_history)